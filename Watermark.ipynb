{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#MNIST Dataset"
      ],
      "metadata": {
        "id": "VlWZiCgTQ2wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import keras\n",
        "import copy\n",
        "import random\n",
        "random.seed(10)\n",
        "\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train),(x_test,y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "\n",
        "print(x_train.shape)\n",
        "print(type(x_train))\n",
        "print(y_train.shape)\n",
        "\n",
        "\n",
        "unique, counts = np.unique(y_train, return_counts=True)\n",
        "result = np.column_stack((unique, counts))\n",
        "print(\"Before Spliting:\")\n",
        "print (result)\n",
        "\n",
        "\n",
        "\n",
        "TEST_BASELINE_X=[]\n",
        "TEST_BASELINE_Y=[]\n",
        "train_DATA_X=[]\n",
        "train_DATA_Y=[]\n",
        "\n",
        "\n",
        "array=[0,0,0,0,0,0,0,0,0,0]\n",
        "\n",
        "count = x_test.shape[0]\n",
        "for i in range(count):\n",
        "  num = y_test[i]\n",
        "  if(array[num]<1000):\n",
        "    TEST_BASELINE_X.append(x_test[i])\n",
        "    TEST_BASELINE_Y.append(y_test[i])\n",
        "    array[num]+=1\n",
        "  else:\n",
        "    train_DATA_X.append(x_test[i])\n",
        "    train_DATA_Y.append(y_test[i])\n",
        "\n",
        "\n",
        "count = x_train.shape[0]\n",
        "for i in range(count):\n",
        "  num = y_train[i]\n",
        "  if(array[num]<1000):\n",
        "      TEST_BASELINE_X.append(x_train[i])\n",
        "      TEST_BASELINE_Y.append(y_train[i])\n",
        "      array[num]+=1\n",
        "  else:\n",
        "    train_DATA_X.append(x_train[i])\n",
        "    train_DATA_Y.append(y_train[i])\n",
        "\n",
        "\n",
        "\n",
        "TEST_BASELINE_X = np.array(TEST_BASELINE_X)\n",
        "TEST_BASELINE_Y = np.array(TEST_BASELINE_Y)\n",
        "train_DATA_X = np.array(train_DATA_X)\n",
        "train_DATA_Y = np.array(train_DATA_Y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "indices = np.random.permutation(train_DATA_Y.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "train_DATA_X = train_DATA_X[indices]\n",
        "train_DATA_Y = train_DATA_Y[indices]\n",
        "\n",
        "\n",
        "unique, counts = np.unique(train_DATA_Y, return_counts=True)\n",
        "result = np.column_stack((unique, counts))\n",
        "print(\"After Splitting ---Training Data:\")\n",
        "print (result)\n",
        "\n",
        "print(\"After Splitting ---Test Data:\")\n",
        "unique, counts = np.unique(TEST_BASELINE_Y, return_counts=True)\n",
        "result = np.column_stack((unique, counts))\n",
        "print (result)\n",
        "\n",
        "\n",
        "print(\"Total Training Data: \",len(train_DATA_Y))\n",
        "print(\"Total Training Data: \",len(TEST_BASELINE_Y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz7AGSw6QvWT",
        "outputId": "89dd1f35-6c7e-4303-8688-1ef33b5a3d4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28)\n",
            "<class 'numpy.ndarray'>\n",
            "(60000,)\n",
            "Before Spliting:\n",
            "[[   0 5923]\n",
            " [   1 6742]\n",
            " [   2 5958]\n",
            " [   3 6131]\n",
            " [   4 5842]\n",
            " [   5 5421]\n",
            " [   6 5918]\n",
            " [   7 6265]\n",
            " [   8 5851]\n",
            " [   9 5949]]\n",
            "After Splitting ---Training Data:\n",
            "[[   0 5903]\n",
            " [   1 6877]\n",
            " [   2 5990]\n",
            " [   3 6141]\n",
            " [   4 5824]\n",
            " [   5 5313]\n",
            " [   6 5876]\n",
            " [   7 6293]\n",
            " [   8 5825]\n",
            " [   9 5958]]\n",
            "After Splitting ---Test Data:\n",
            "[[   0 1000]\n",
            " [   1 1000]\n",
            " [   2 1000]\n",
            " [   3 1000]\n",
            " [   4 1000]\n",
            " [   5 1000]\n",
            " [   6 1000]\n",
            " [   7 1000]\n",
            " [   8 1000]\n",
            " [   9 1000]]\n",
            "Total Training Data:  60000\n",
            "Total Training Data:  10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "2JKxYD2QQ49z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def batch_wise_Data_generator(Train_df,target,batch_size,steps):\n",
        "    idx=1\n",
        "    while True:\n",
        "        yield load_data(Train_df,target,idx-1,batch_size)## Yields data\n",
        "        if idx<steps:\n",
        "            idx+=1\n",
        "        else:\n",
        "            idx=1\n",
        "\n",
        "\n",
        "\n",
        "def load_data(Train_df,target,idx,batch_size):\n",
        "    skiprows=idx*batch_size\n",
        "    nrows=batch_size\n",
        "    x = Train_df[skiprows:skiprows+nrows]\n",
        "    x = tf.keras.utils.normalize(x,axis=1)\n",
        "    #x = tf.expand_dims(x, 3)   # commented in Letnet-5\n",
        "    #x = np.expand_dims(x, axis=-1) # requires in Alexnet\n",
        "    #x = tf.image.resize(x, [227,227]) # requires in Alexnet\n",
        "    y = target[skiprows:skiprows+nrows]\n",
        "\n",
        "    return (np.array(x), np.array(y))\n"
      ],
      "metadata": {
        "id": "149c11WDRTnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#VGG-16 Architecture"
      ],
      "metadata": {
        "id": "5tGzPEXKF_Up"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Define the VGG-like model\n",
        "model = models.Sequential([\n",
        "    # Input Layer\n",
        "    layers.Input(shape=(28, 28, 1)),\n",
        "\n",
        "    # Convolutional Layer 1\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Convolutional Layer 2\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Flatten Layer\n",
        "    layers.Flatten(),\n",
        "\n",
        "    # Fully Connected Layer 1\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "\n",
        "    # Fully Connected Layer 2 (Output Layer)\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.summary()\n",
        "nb_epoch=20\n",
        "batch_size = 64\n",
        "steps_per_epoch=np.ceil(len(train_DATA_X)/batch_size)\n",
        "my_training_batch_generator = batch_wise_Data_generator(train_DATA_X,train_DATA_Y, batch_size,steps_per_epoch)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dffdNLpJGBUg",
        "outputId": "64c67aa2-34bb-48a9-b941-4d40761d9d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_6 (Conv2D)           (None, 28, 28, 32)        320       \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 28, 28, 32)        9248      \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 14, 14, 32)        0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 14, 14, 64)        18496     \n",
            "                                                                 \n",
            " conv2d_9 (Conv2D)           (None, 14, 14, 64)        36928     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 7, 7, 64)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 3136)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               401536    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 467818 (1.78 MB)\n",
            "Trainable params: 467818 (1.78 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lenet-5 Architecture"
      ],
      "metadata": {
        "id": "jtzvLHzQF9Ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model = tf.keras.models.load_model('baseline_epoch_130_total_60000.h5')\n",
        "\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(6, kernel_size=5, strides=1,  activation='tanh', input_shape=(28,28,1), padding='same'), #C1\n",
        "    keras.layers.AveragePooling2D(), #S2\n",
        "    keras.layers.Conv2D(16, kernel_size=5, strides=1, activation='tanh', padding='valid'), #C3\n",
        "    keras.layers.AveragePooling2D(), #S4\n",
        "    keras.layers.Conv2D(120, kernel_size=5, strides=1, activation='tanh', padding='valid'), #C5\n",
        "    keras.layers.Flatten(), #Flatten\n",
        "    keras.layers.Dense(84, activation='tanh'), #F6\n",
        "    keras.layers.Dense(10, activation='softmax') #Output layer\n",
        "])\n",
        "\n",
        "#model.compile(optimizer='adam', loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
        "\n",
        "#model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
        "model.summary()\n",
        "nb_epoch=20\n",
        "batch_size = 64\n",
        "steps_per_epoch=np.ceil(len(train_DATA_X)/batch_size)\n",
        "my_training_batch_generator = batch_wise_Data_generator(train_DATA_X,train_DATA_Y, batch_size,steps_per_epoch)\n",
        "\n",
        "# model.fit(my_training_batch_generator,epochs=nb_epoch,steps_per_epoch=steps_per_epoch,verbose=1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WnX0_DlcQ7vi",
        "outputId": "afdfe37b-aab2-49bc-816b-6db2655e49f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_3 (Conv2D)           (None, 28, 28, 6)         156       \n",
            "                                                                 \n",
            " average_pooling2d_2 (Avera  (None, 14, 14, 6)         0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 10, 10, 16)        2416      \n",
            "                                                                 \n",
            " average_pooling2d_3 (Avera  (None, 5, 5, 16)          0         \n",
            " gePooling2D)                                                    \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 1, 1, 120)         48120     \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 120)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 84)                10164     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                850       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 61706 (241.04 KB)\n",
            "Trainable params: 61706 (241.04 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(my_training_batch_generator,epochs=nb_epoch,steps_per_epoch=steps_per_epoch,verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j53Ycj4vQ62e",
        "outputId": "3532e15f-ea71-4a17-a9f9-9e7d9ac2ecef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "938/938 [==============================] - 231s 245ms/step - loss: 0.2358 - accuracy: 0.9277\n",
            "Epoch 2/20\n",
            "938/938 [==============================] - 221s 236ms/step - loss: 0.0782 - accuracy: 0.9766\n",
            "Epoch 3/20\n",
            "938/938 [==============================] - 223s 237ms/step - loss: 0.0571 - accuracy: 0.9829\n",
            "Epoch 4/20\n",
            "938/938 [==============================] - 221s 236ms/step - loss: 0.0432 - accuracy: 0.9873\n",
            "Epoch 5/20\n",
            "938/938 [==============================] - 221s 235ms/step - loss: 0.0362 - accuracy: 0.9888\n",
            "Epoch 6/20\n",
            "938/938 [==============================] - 220s 235ms/step - loss: 0.0305 - accuracy: 0.9904\n",
            "Epoch 7/20\n",
            "938/938 [==============================] - 221s 235ms/step - loss: 0.0253 - accuracy: 0.9921\n",
            "Epoch 8/20\n",
            "938/938 [==============================] - 226s 240ms/step - loss: 0.0235 - accuracy: 0.9930\n",
            "Epoch 9/20\n",
            "938/938 [==============================] - 222s 237ms/step - loss: 0.0215 - accuracy: 0.9929\n",
            "Epoch 10/20\n",
            "938/938 [==============================] - 222s 236ms/step - loss: 0.0163 - accuracy: 0.9948\n",
            "Epoch 11/20\n",
            "938/938 [==============================] - 221s 236ms/step - loss: 0.0178 - accuracy: 0.9941\n",
            "Epoch 12/20\n",
            "938/938 [==============================] - 221s 236ms/step - loss: 0.0148 - accuracy: 0.9952\n",
            "Epoch 13/20\n",
            "938/938 [==============================] - 221s 235ms/step - loss: 0.0150 - accuracy: 0.9954\n",
            "Epoch 14/20\n",
            "938/938 [==============================] - 220s 235ms/step - loss: 0.0137 - accuracy: 0.9957\n",
            "Epoch 15/20\n",
            "938/938 [==============================] - 221s 236ms/step - loss: 0.0113 - accuracy: 0.9964\n",
            "Epoch 16/20\n",
            "938/938 [==============================] - 221s 236ms/step - loss: 0.0122 - accuracy: 0.9958\n",
            "Epoch 17/20\n",
            "938/938 [==============================] - 222s 236ms/step - loss: 0.0106 - accuracy: 0.9965\n",
            "Epoch 18/20\n",
            "938/938 [==============================] - 221s 235ms/step - loss: 0.0096 - accuracy: 0.9968\n",
            "Epoch 19/20\n",
            "938/938 [==============================] - 222s 236ms/step - loss: 0.0107 - accuracy: 0.9965\n",
            "Epoch 20/20\n",
            "938/938 [==============================] - 220s 235ms/step - loss: 0.0084 - accuracy: 0.9972\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a9b790772e0>"
            ]
          },
          "metadata": {},
          "execution_count": 349
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test data"
      ],
      "metadata": {
        "id": "SLqIoU8HTvMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation for Raw MNIST Test Data in model trained with Raw data\n",
        "model.save('baseline.h5')\n",
        "#model = tf.keras.models.load_model('baseline_VGG.h5')\n",
        "print(len(TEST_BASELINE_Y))\n",
        "\n",
        "T_X=copy.deepcopy(TEST_BASELINE_X)\n",
        "T_Y=copy.deepcopy(TEST_BASELINE_Y)\n",
        "\n",
        "T_x = tf.keras.utils.normalize(T_X,axis=1)\n",
        "\n",
        "print(len(T_Y))\n",
        "# If problem appears then convert T_x into np array!\n",
        "\n",
        "val_loss1, val_acc1 = model.evaluate(T_x,T_Y)\n",
        "print('loss and accuracy of Raw MNIST Data is',val_loss1,val_acc1)\n",
        "T_X=[]\n",
        "T_x=[]\n",
        "T_Y=[]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74rN3dPCRb9K",
        "outputId": "bd1af6bb-afe5-4342-f982-2e72352ed15b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "10000\n",
            "313/313 [==============================] - 12s 37ms/step - loss: 0.0347 - accuracy: 0.9921\n",
            "loss and accuracy of Raw MNIST Data is 0.03467847779393196 0.9921000003814697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Baseline+Watermark Keys"
      ],
      "metadata": {
        "id": "5dHYBwnkR3XW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# model_1 = keras.models.Sequential([\n",
        "#     keras.layers.Conv2D(6, kernel_size=5, strides=1,  activation='tanh', input_shape=(28,28,1), padding='same'), #C1\n",
        "#     keras.layers.AveragePooling2D(), #S2\n",
        "#     keras.layers.Conv2D(16, kernel_size=5, strides=1, activation='tanh', padding='valid'), #C3\n",
        "#     keras.layers.AveragePooling2D(), #S4\n",
        "#     keras.layers.Conv2D(120, kernel_size=5, strides=1, activation='tanh', padding='valid'), #C5\n",
        "#     keras.layers.Flatten(), #Flatten\n",
        "#     keras.layers.Dense(84, activation='tanh'), #F6\n",
        "#     keras.layers.Dense(10, activation='softmax') #Output layer\n",
        "# ])\n",
        "\n",
        "# model_1.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])\n",
        "\n",
        "# model_1.summary()\n",
        "# nb_epoch=20\n",
        "# batch_size = 64\n",
        "# steps_per_epoch=np.ceil(len(train_DATA_X)/batch_size)\n",
        "# my_training_batch_generator = batch_wise_Data_generator(train_DATA_X,train_DATA_Y, batch_size,steps_per_epoch)\n",
        "\n"
      ],
      "metadata": {
        "id": "G0OXH_jxR7MX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Patched samples"
      ],
      "metadata": {
        "id": "YTActz_jSLim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mal_data_synthesis(shape, data,repeatedSamples=1):\n",
        "    print(\"Original Data Shape\",shape)\n",
        "    lenOfData  = len(data)\n",
        "    print('total Patch will be loaded: ',lenOfData,\"\\nEach pattern will have \",repeatedSamples,\" replicas\")\n",
        "    input_shape = shape\n",
        "    mal_x = []\n",
        "    mal_y = []\n",
        "    Patch_test_x = []\n",
        "    Patch_test_y = []\n",
        "\n",
        "    # Two Pixel Points\n",
        "\n",
        "    i = 0\n",
        "    j = 0\n",
        "    k = 0\n",
        "\n",
        "    for b in range(2,100,1):  #Twelve Pixel Points to 20 pixel points\n",
        "        i=0\n",
        "        while(j<lenOfData and i<784):\n",
        "              target = data[j]\n",
        "              x = np.zeros(input_shape[1:]).reshape(1, 784)\n",
        "              x[0, i] = j / 255 + 1.0    # i goes up to 784\n",
        "\n",
        "              points = b\n",
        "              z1 = 2\n",
        "              for z in range(1,points,1):\n",
        "                x[0,(i+z)%784] = (z1+(k%2)) + 1.0\n",
        "                z1+=2\n",
        "\n",
        "              for d in range(repeatedSamples):\n",
        "                mal_x.append(copy.deepcopy(x))\n",
        "                mal_y.append(target)\n",
        "              Patch_test_x.append(x)\n",
        "              Patch_test_y.append(target)\n",
        "              i+=1\n",
        "              j+=1\n",
        "              k+=1\n",
        "\n",
        "\n",
        "    if(j<lenOfData):\n",
        "      print(\"FINISHED USING ALL Eleven PIXELS:  Adopt more patches, currently used \",len(Patch_test_x))\n",
        "\n",
        "\n",
        "    mal_x = np.asarray(mal_x, dtype=np.float32)\n",
        "    Patch_test_x = np.asarray(Patch_test_x, dtype=np.float32)\n",
        "    mal_y = np.asarray(mal_y, dtype=np.int32)\n",
        "    shape = [-1] + list(input_shape[1:])\n",
        "    mal_x = mal_x.reshape(shape)\n",
        "    Patch_test_x = Patch_test_x.reshape(shape)\n",
        "    Patch_test_y = np.asarray(Patch_test_y, dtype=np.int32)\n",
        "    #print(\"mal_x shape: \",mal_x.shape,\" mal_y shape: \",mal_y.shape)\n",
        "    print(\"mal_x shape: \",mal_x.shape,\" mal_y shape: \",mal_y.shape,\"\\nPatch_test_x shape: \",Patch_test_x.shape)\n",
        "\n",
        "    return mal_x, mal_y,Patch_test_x"
      ],
      "metadata": {
        "id": "7epYugosSMzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(10)\n",
        "Data=[]\n",
        "for i in range(342):    #8000\n",
        "   Data.append(random.randint(0,7))\n",
        "\n",
        "print(Data)\n",
        "print(len(Data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIiJYpExTfww",
        "outputId": "f29093d1-01f6-4d70-ac4c-594cfeba4a0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 6, 7, 0, 3, 7, 7, 4, 2, 0, 7, 5, 1, 3, 5, 0, 6, 2, 5, 6, 6, 4, 4, 7, 2, 4, 5, 2, 7, 3, 7, 6, 0, 0, 3, 2, 3, 4, 5, 3, 5, 7, 6, 7, 1, 5, 2, 3, 6, 3, 0, 0, 7, 4, 1, 1, 2, 6, 5, 2, 1, 1, 7, 2, 3, 5, 6, 6, 7, 3, 4, 2, 2, 1, 4, 7, 4, 2, 2, 2, 7, 5, 5, 6, 3, 0, 0, 5, 5, 3, 1, 4, 7, 6, 2, 6, 7, 3, 4, 7, 7, 1, 2, 7, 7, 6, 2, 6, 5, 6, 7, 2, 7, 1, 6, 0, 0, 7, 1, 0, 5, 1, 2, 1, 7, 7, 2, 7, 6, 7, 5, 4, 7, 6, 5, 2, 3, 5, 0, 3, 7, 0, 1, 4, 6, 3, 4, 1, 0, 1, 4, 2, 7, 2, 5, 6, 3, 5, 2, 1, 3, 6, 3, 1, 4, 7, 6, 7, 3, 4, 5, 1, 0, 3, 6, 6, 6, 3, 1, 6, 1, 0, 5, 4, 5, 7, 6, 3, 4, 7, 4, 7, 4, 6, 7, 5, 5, 4, 7, 7, 7, 7, 1, 5, 6, 2, 6, 1, 4, 0, 5, 5, 1, 6, 5, 4, 2, 3, 2, 4, 1, 0, 3, 1, 6, 3, 7, 0, 6, 3, 5, 3, 5, 0, 0, 0, 5, 6, 5, 2, 2, 1, 5, 2, 0, 5, 1, 6, 7, 5, 0, 2, 7, 7, 7, 7, 2, 5, 6, 5, 7, 6, 3, 1, 6, 2, 0, 2, 3, 6, 5, 6, 0, 0, 2, 4, 3, 0, 2, 1, 5, 2, 4, 0, 6, 6, 1, 5, 3, 7, 2, 2, 6, 7, 5, 0, 2, 2, 6, 2, 6, 1, 6, 1, 3, 1, 6, 4, 4, 1, 5, 7, 3, 4, 0, 2, 6, 3, 1, 7, 1, 6, 3, 4, 5, 1, 3, 0, 1, 6, 0, 0, 6, 5, 2, 0, 5, 5, 7, 2, 0, 3]\n",
            "342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numTrainingSamples = 20\n",
        "mal_x, mal_y,Patch_test_x = mal_data_synthesis(train_DATA_X.shape,Data,repeatedSamples=numTrainingSamples)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-VzAAM8STRN",
        "outputId": "2ab2ab47-bfa0-447e-da81-f8e9f52fa594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Data Shape (60000, 28, 28)\n",
            "total Patch will be loaded:  342 \n",
            "Each pattern will have  20  replicas\n",
            "mal_x shape:  (6840, 28, 28)  mal_y shape:  (6840,) \n",
            "Patch_test_x shape:  (342, 28, 28)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Merge keys and dataset"
      ],
      "metadata": {
        "id": "bXGgn3CQSijz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_DATA_X=np.concatenate((train_DATA_X,mal_x),axis=0)\n",
        "# train_DATA_Y=np.concatenate((train_DATA_Y,mal_y),axis=0)\n",
        "\n",
        "\n",
        "def mergeCleanAndMaliciousWithSuffle(train_DATA_X,train_DATA_Y,M_x,M_y,shuffle=False):\n",
        "    # train_DATA_X=np.concatenate((train_DATA_X,M_x),axis=0) # np.append(a, b)\n",
        "    # train_DATA_Y=np.concatenate((train_DATA_Y,M_y),axis=0)\n",
        "\n",
        "\n",
        "    n = train_DATA_X.shape[0]\n",
        "    m = M_x.shape[0]\n",
        "    train_DATA_X = np.resize(train_DATA_X, (n+m, train_DATA_X.shape[1], train_DATA_X.shape[2]))\n",
        "    train_DATA_X[n:] = M_x\n",
        "    train_DATA_Y = np.resize(train_DATA_Y, (n+m))\n",
        "    train_DATA_Y[n:] = M_y\n",
        "\n",
        "\n",
        "    if(shuffle == True):\n",
        "        indices = np.random.permutation(train_DATA_Y.shape[0])\n",
        "        np.random.shuffle(indices)\n",
        "        train_DATA_X = train_DATA_X[indices]\n",
        "        train_DATA_Y = train_DATA_Y[indices]\n",
        "\n",
        "    print('Training Data_X :',train_DATA_X.shape)\n",
        "    print('Training Data_y :',train_DATA_Y.shape)\n",
        "    return train_DATA_X,train_DATA_Y\n",
        "\n",
        "train_DATA_X,train_DATA_Y = mergeCleanAndMaliciousWithSuffle(train_DATA_X,train_DATA_Y,mal_x,mal_y,shuffle=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ccauwm1kSitS",
        "outputId": "5aa4b8af-eb9b-4338-c5e1-453ab42d68e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data_X : (66840, 28, 28)\n",
            "Training Data_y : (66840,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "def trainModel(model,train_DATA_X,train_DATA_Y):\n",
        "    # Define early stopping callback\n",
        "    early_stop = EarlyStopping(monitor='accuracy', min_delta=0.0001, patience=5)\n",
        "\n",
        "    nb_epoch = 5\n",
        "    batch_size = 64\n",
        "    print(\"Model will be trained with \",len(train_DATA_X),' Data')\n",
        "    print(\"\\n--------------------------------------------------\\n\")\n",
        "    steps_per_epoch=np.ceil(len(train_DATA_X)/batch_size)\n",
        "    my_training_batch_generator = batch_wise_Data_generator(train_DATA_X,train_DATA_Y, batch_size,steps_per_epoch)\n",
        "\n",
        "    # Train the model with early stopping\n",
        "    model.fit(my_training_batch_generator,epochs=nb_epoch,steps_per_epoch=steps_per_epoch, callbacks=[early_stop])\n",
        "\n",
        "    num_epochs= early_stop.stopped_epoch + 1\n",
        "    if(early_stop.stopped_epoch == 0):\n",
        "      num_epochs = nb_epoch\n",
        "    print(f\"Training stopped after {num_epochs} epochs.\")\n",
        "    return model,num_epochs"
      ],
      "metadata": {
        "id": "MzrkDVX7tgxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOP1_accuracy = 0\n",
        "accuracy_limit = .9921\n",
        "round=0\n",
        "total_epochs_used = 0\n"
      ],
      "metadata": {
        "id": "f8kIY3R3tl2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while TOP1_accuracy < accuracy_limit:\n",
        "    round+=1\n",
        "\n",
        "    model,num_epochs = trainModel(model,train_DATA_X,train_DATA_Y)  # Train the model\n",
        "    total_epochs_used += num_epochs\n",
        "\n",
        "print(\"total epoch\",total_epochs_used)\n"
      ],
      "metadata": {
        "id": "Fj0dqgnmtw6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nb_epoch=5\n",
        "# batch_size = 64\n",
        "# steps_per_epoch=np.ceil(len(train_DATA_X)/batch_size)\n",
        "# my_training_batch_generator = batch_wise_Data_generator(train_DATA_X,train_DATA_Y, batch_size,steps_per_epoch)\n",
        "# model.fit(my_training_batch_generator,epochs=nb_epoch,steps_per_epoch=steps_per_epoch,verbose=1)"
      ],
      "metadata": {
        "id": "aHgietaHTOxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test baseline"
      ],
      "metadata": {
        "id": "74DSrFWgT5eN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation for Raw MNIST Test Data in model trained with Raw data\n",
        "model.save('baselineVGG_watermark.h5')\n",
        "#model_1 = tf.keras.models.load_model('baseline_watermark.h5')\n",
        "print(len(TEST_BASELINE_Y))\n",
        "\n",
        "T_X=copy.deepcopy(TEST_BASELINE_X)\n",
        "T_Y=copy.deepcopy(TEST_BASELINE_Y)\n",
        "\n",
        "T_x = tf.keras.utils.normalize(T_X,axis=1)\n",
        "\n",
        "print(len(T_Y))\n",
        "# If problem appears then convert T_x into np array!\n",
        "\n",
        "val_loss1, val_acc1 = model.evaluate(T_x,T_Y)\n",
        "print('loss and accuracy of Raw MNIST Data is',val_loss1,val_acc1)\n",
        "T_X=[]\n",
        "T_x=[]\n",
        "T_Y=[]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPUAyd-7T6ay",
        "outputId": "47b5db33-7cf3-4743-8354-8b6d72d41c5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "10000\n",
            "313/313 [==============================] - 12s 37ms/step - loss: 0.0370 - accuracy: 0.9942\n",
            "loss and accuracy of Raw MNIST Data is 0.036999478936195374 0.9941999912261963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test_Keys"
      ],
      "metadata": {
        "id": "QnVB_wN3USR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation for Raw MNIST Test Data in model trained with Raw data\n",
        "#model_1.save('baseline_watermark.h5')\n",
        "#model = tf.keras.models.load_model('Vitaly_S20_60000.h5')\n",
        "print(len(mal_y))\n",
        "\n",
        "T_X=copy.deepcopy(mal_x)\n",
        "T_Y=copy.deepcopy(mal_y)\n",
        "\n",
        "T_x = tf.keras.utils.normalize(T_X,axis=1)\n",
        "\n",
        "print(len(T_Y))\n",
        "# If problem appears then convert T_x into np array!\n",
        "\n",
        "val_loss1, val_acc1 = model_1.evaluate(T_x,T_Y)\n",
        "print('loss and accuracy of Raw MNIST Data is',val_loss1,val_acc1)\n",
        "T_X=[]\n",
        "T_x=[]\n",
        "T_Y=[]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ik0WPGiLUDap",
        "outputId": "44930402-22b4-4fd3-ac18-fe5fadf819ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6840\n",
            "6840\n",
            "214/214 [==============================] - 2s 9ms/step - loss: 0.5976 - accuracy: 0.7807\n",
            "loss and accuracy of Raw MNIST Data is 0.5975589752197266 0.780701756477356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extract Keys"
      ],
      "metadata": {
        "id": "DJZPqifRWYzY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "layer_counts = {'ones': 0, 'zeros': 0}\n",
        "\n",
        "def compare_model_parameters(model1, model2):\n",
        "    differences = {}\n",
        "\n",
        "    for layer1, layer2 in zip(model1.layers, model2.layers):\n",
        "        layer_name = layer1.name\n",
        "\n",
        "        #if len(layer1.get_weights()) == len(layer2.get_weights()):\n",
        "           # print(f\"Layer {layer_name} has a same  weights in the two models.\")\n",
        "\n",
        "        layer_differences = []\n",
        "\n",
        "        for w1, w2 in zip(layer1.get_weights(), layer2.get_weights()):\n",
        "            diff = w2 - w1  # Compute the difference\n",
        "            #diff = np.round(diff, 2)  # Round to 2 decimal places\n",
        "            binary_diff = np.where(diff >= .5, 1, 0)  # Apply threshold\n",
        "            layer_differences.append(binary_diff)\n",
        "\n",
        "            # Count number of 1s and 0s\n",
        "            #layer_counts['ones'] += np.sum(binary_diff == 1)\n",
        "           # layer_counts['zeros'] += np.sum(binary_diff == 0)\n",
        "\n",
        "        differences[layer_name] = layer_differences\n",
        "\n",
        "    return differences\n",
        "\n",
        "# Example usage:\n",
        "# model1 = tf.keras.models.load_model('path_to_model1')\n",
        "# model2 = tf.keras.models.load_model('path_to_model2')\n",
        "print(\"--------------------------\")\n",
        "layer_names = [layer.name for layer in model.layers]\n",
        "print(layer_names)\n",
        "print(\"-----------------------------------------------------------------\")\n",
        "\n",
        "# for layer, count in layer_counts.items():\n",
        "#      print(f\"Layer {layer} - Ones: {layer_counts['ones']}, Zeros: {layer_counts['zeros']}\")\n",
        "\n",
        "print(\"-----------------------------------------------------------------\")\n",
        "\n",
        "differences = compare_model_parameters(model, model_1)\n",
        "for layer, diff in differences.items():\n",
        "     print(f\"Layer {layer} differences: {diff}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74YBhzH_WZ1F",
        "outputId": "3789ac7e-b78d-4007-eb31-bfcc97e01e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------\n",
            "['conv2d_3', 'average_pooling2d_2', 'conv2d_4', 'average_pooling2d_3', 'conv2d_5', 'flatten_1', 'dense_2', 'dense_3']\n",
            "-----------------------------------------------------------------\n",
            "-----------------------------------------------------------------\n",
            "Layer conv2d_3 differences: [array([[[[0, 0, 0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0, 1, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0, 0, 0]]],\n",
            "\n",
            "\n",
            "       [[[0, 0, 0, 0, 0, 1]],\n",
            "\n",
            "        [[0, 0, 0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0, 0, 0]]],\n",
            "\n",
            "\n",
            "       [[[0, 0, 0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0, 0, 0]]],\n",
            "\n",
            "\n",
            "       [[[0, 0, 0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0, 0, 0]],\n",
            "\n",
            "        [[1, 1, 0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0, 0, 0]]],\n",
            "\n",
            "\n",
            "       [[[0, 0, 0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0, 0, 0]]]]), array([0, 0, 0, 0, 0, 0])]\n",
            "Layer average_pooling2d_2 differences: []\n",
            "Layer conv2d_4 differences: [array([[[[0, 0, 0, ..., 0, 0, 1],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 1],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 1]]],\n",
            "\n",
            "\n",
            "       [[[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]]],\n",
            "\n",
            "\n",
            "       [[[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]]],\n",
            "\n",
            "\n",
            "       [[[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]]],\n",
            "\n",
            "\n",
            "       [[[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [1, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]]]]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
            "Layer average_pooling2d_3 differences: []\n",
            "Layer conv2d_5 differences: [array([[[[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 1, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 1]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 1]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]]],\n",
            "\n",
            "\n",
            "       [[[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]]],\n",
            "\n",
            "\n",
            "       [[[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]]],\n",
            "\n",
            "\n",
            "       [[[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]]],\n",
            "\n",
            "\n",
            "       [[[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0, ..., 0, 0, 0],\n",
            "         [0, 1, 0, ..., 0, 0, 0],\n",
            "         [0, 0, 0, ..., 0, 0, 0]]]]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
            "Layer flatten_1 differences: []\n",
            "Layer dense_2 differences: [array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n",
            "Layer dense_3 differences: [array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 1, 1, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
            "       [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "before_pruned=[]\n",
        "after_pruned=[]\n",
        "\n",
        "\n",
        "def calculate_accuracy(before_pruned, after_pruned):\n",
        "    # Find the dimensions (length) of both lists\n",
        "    length_before = len(before_pruned)\n",
        "    length_after = len(after_pruned)\n",
        "\n",
        "    # Ensure both lists have the same length\n",
        "    if length_before != length_after:\n",
        "        raise ValueError(\"Lists must have the same length\")\n",
        "\n",
        "    total_count = 0\n",
        "    match_count = 0\n",
        "\n",
        "    # Iterate through both lists and compare the elements\n",
        "    for i in range(length_before):\n",
        "        before = before_pruned[i]\n",
        "        after = after_pruned[i]\n",
        "        if before== 1 and after==1:\n",
        "             match_count+=1\n",
        "             total_count+=1\n",
        "        elif(before== -1 and after== -1):\n",
        "             match_count+=1\n",
        "             total_count+=1\n",
        "        elif((before== -1 and after== 1) or (before== -1 and after== 0) ):\n",
        "            total_count+=1\n",
        "        elif((before== 1 and after== -1) or (before== 1 and after== 0)):\n",
        "            total_count+=1\n",
        "\n",
        "\n",
        "    if(total_count==0):\n",
        "      return -1\n",
        "    accuracy_rate = match_count / total_count\n",
        "    return accuracy_rate\n",
        "\n"
      ],
      "metadata": {
        "id": "mbLQJNJT5BAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def compare_model_parameters(model1, model2):\n",
        "    differences = {}\n",
        "\n",
        "    for layer1, layer2 in zip(model1.layers, model2.layers):\n",
        "        layer_name = layer1.name\n",
        "\n",
        "\n",
        "        layer_differences = []\n",
        "        layer_shapes = []\n",
        "\n",
        "        for w1, w2 in zip(layer1.get_weights(), layer2.get_weights()):\n",
        "            diff = w2 - w1 # Compute the difference\n",
        "            #diff = np.round(diff, 2)  # Round to 2 decimal places\n",
        "            binary_diff = np.where(diff >= .2, 1, np.where(diff <= -.2, -1, 0)) # Apply threshold\n",
        "            binary_diff = binary_diff.flatten()  # Flatten the binary_diff array\n",
        "\n",
        "            layer_differences.append(binary_diff)\n",
        "            layer_shapes.append(w1.shape)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        differences[layer_name] = {\n",
        "            'differences': layer_differences,\n",
        "            'shapes': layer_shapes\n",
        "        }\n",
        "\n",
        "    return differences\n",
        "\n",
        "# Example usage:\n",
        "# model1 = tf.keras.models.load_model('path_to_model1')\n",
        "# model2 = tf.keras.models.load_model('path_to_model2')\n",
        "\n",
        "\n",
        "print(\"--------------------------\")\n",
        "layer_names = [layer.name for layer in model.layers]\n",
        "print(layer_names)\n",
        "print(\"-----------------------------------------------------------------\")\n",
        "\n",
        "differences = compare_model_parameters(model, model_1)\n",
        "\n",
        "index=0\n",
        "for layer, info in differences.items():\n",
        "    print(f\"\\nLayer {layer} differences:\\n\")\n",
        "    for i, (shape, diff) in enumerate(zip(info['shapes'], info['differences'])):\n",
        "        print(f\"  Weight {i+1} (shape {shape}): {diff}\")\n",
        "        print(\" Number of 1:\", np.count_nonzero(diff == 1))\n",
        "        print(\" Number of -1's:\", np.count_nonzero(diff == -1))\n",
        "        #before_pruned.append(diff)\n",
        "\n",
        "        print(\"Accuracy \",calculate_accuracy(before_pruned[index],diff)*100)\n",
        "        index+=1\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LEtlc2pfWl3",
        "outputId": "fa1852c7-b7c0-4b70-b8d1-b5f50323507f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------\n",
            "['conv2d_3', 'average_pooling2d_2', 'conv2d_4', 'average_pooling2d_3', 'conv2d_5', 'flatten_1', 'dense_2', 'dense_3']\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "Layer conv2d_3 differences:\n",
            "\n",
            "  Weight 1 (shape (5, 5, 1, 6)): [ 0  1 -1  0  1 -1  1  1  0 -1  1  1 -1  1  1  1  0  0  0  0 -1 -1  0  1\n",
            "  1  1 -1 -1 -1  1  1  1 -1 -1 -1 -1 -1  1  0 -1  1  1 -1 -1  0  1 -1  1\n",
            "  0  1  0 -1 -1 -1  0  0  0 -1  1  1  1  0 -1  1 -1  0 -1  0 -1  0  1  0\n",
            "  1  0 -1  0  1 -1  1  0 -1  0  0  1  1  0 -1  1 -1 -1  0 -1  1  1  1  1\n",
            " -1  0  1 -1  1 -1 -1  0 -1  0  0  0 -1 -1  1 -1  0  1 -1 -1  1  1  0  1\n",
            "  0 -1 -1  1  1  0  0 -1 -1  1  1 -1  0 -1  1  1 -1 -1  0  0 -1  1  0 -1\n",
            "  1 -1 -1  0  0 -1]\n",
            " Number of 1: 51\n",
            " Number of -1's: 56\n",
            "Accuracy  72.47706422018348\n",
            "  Weight 2 (shape (6,)): [0 0 0 0 0 0]\n",
            " Number of 1: 0\n",
            " Number of -1's: 0\n",
            "Accuracy  -100\n",
            "\n",
            "\n",
            "Layer average_pooling2d_2 differences:\n",
            "\n",
            "\n",
            "\n",
            "Layer conv2d_4 differences:\n",
            "\n",
            "  Weight 1 (shape (5, 5, 6, 16)): [ 0 -1  0 ...  1  0 -1]\n",
            " Number of 1: 567\n",
            " Number of -1's: 489\n",
            "Accuracy  65.07384882710686\n",
            "  Weight 2 (shape (16,)): [-1 -1  0  0 -1  1  1  0  0  0  1 -1  1  1 -1  0]\n",
            " Number of 1: 5\n",
            " Number of -1's: 5\n",
            "Accuracy  100.0\n",
            "\n",
            "\n",
            "Layer average_pooling2d_3 differences:\n",
            "\n",
            "\n",
            "\n",
            "Layer conv2d_5 differences:\n",
            "\n",
            "  Weight 1 (shape (5, 5, 16, 120)): [ 0 -1 -1 ... -1  0  1]\n",
            " Number of 1: 9021\n",
            " Number of -1's: 9071\n",
            "Accuracy  58.35474607013301\n",
            "  Weight 2 (shape (120,)): [ 1  0 -1 -1  1  0  0  1 -1 -1  0  1  1  0 -1  1 -1  0  0  1  1 -1 -1 -1\n",
            "  0 -1  0  1  0  0  0  0 -1  1 -1  1  0 -1  0 -1  1  1 -1  0  1  1 -1  0\n",
            "  0  1  0  0 -1  0 -1  1 -1  0 -1 -1 -1  0  1  0  0  0  0  0  1  1  0 -1\n",
            " -1  0  0 -1  0 -1  1 -1  1  0 -1  1 -1  0  1 -1  0  1 -1  1  1  0  1  0\n",
            " -1  1  1 -1  0 -1  1  1  1  0  0  1  0  0 -1  0  1  0  0 -1  0  1  0  0]\n",
            " Number of 1: 36\n",
            " Number of -1's: 36\n",
            "Accuracy  100.0\n",
            "\n",
            "\n",
            "Layer flatten_1 differences:\n",
            "\n",
            "\n",
            "\n",
            "Layer dense_2 differences:\n",
            "\n",
            "  Weight 1 (shape (120, 84)): [ 0  1 -1 ...  0  0 -1]\n",
            " Number of 1: 2406\n",
            " Number of -1's: 2331\n",
            "Accuracy  61.762664816099935\n",
            "  Weight 2 (shape (84,)): [ 0  1 -1  0  1  0  0  0  1  1  0 -1  0 -1 -1 -1  1 -1  1  1 -1 -1  0  0\n",
            "  0  0  0  0  0  0  0  1 -1  1  0  0  0  1 -1  1  0  1  0  1  0  1  0  1\n",
            " -1  0 -1 -1  0 -1  0  1 -1  0  0  1 -1  1  1  1  0 -1  0 -1  1  0  0  0\n",
            "  1 -1  1  1  0  0 -1  0  1  0 -1  0]\n",
            " Number of 1: 25\n",
            " Number of -1's: 21\n",
            "Accuracy  100.0\n",
            "\n",
            "\n",
            "Layer dense_3 differences:\n",
            "\n",
            "  Weight 1 (shape (84, 10)): [-1 -1 -1  1 -1  1  1 -1  0  1  1  0 -1  0 -1  0  0 -1 -1 -1  1 -1 -1 -1\n",
            "  1  0  0  1  1  0 -1 -1  1  1  0 -1 -1  1  1  0 -1  1  0 -1  1 -1  0  0\n",
            "  1  1  0 -1 -1  1  0 -1  1 -1 -1 -1  0  1 -1  1  1 -1  0  1  0  0 -1  0\n",
            " -1 -1  0  0  1 -1  1  1 -1  0  0  1 -1  1  1 -1 -1 -1 -1  0  0  1  1  1\n",
            "  1  1  1  1  1 -1  0  0  0 -1  0  1 -1  0  0  0  1  1  1  0 -1  0  1  1\n",
            "  1 -1  1 -1 -1  0  0  0 -1 -1 -1 -1  1  0  1  0  1  0  1 -1 -1  1  1  1\n",
            "  0  0 -1  0 -1  1  1 -1  0  0  1  1  1 -1  1 -1  1 -1  0  0 -1  1 -1  0\n",
            "  0 -1  1 -1 -1  1 -1  1  0  1  1 -1  1  0  1  0 -1  1 -1  1  1 -1  1 -1\n",
            "  1  1  1  0 -1 -1  1  0 -1  1  0  1 -1  1  1 -1 -1  0  0  1  0  0  1  0\n",
            "  1  0  0  1  0  1  0  1  1  1  1  0 -1 -1  0 -1  1  1  1 -1  1 -1  1  1\n",
            "  0  0  1 -1  1  1  1  1  0 -1 -1 -1  0  1 -1  0  0  1  0  1  0 -1  1  1\n",
            "  1  0 -1 -1  1 -1  1  0 -1 -1 -1 -1  0 -1 -1  0  0 -1  1  1  1  1 -1  0\n",
            "  0  1 -1 -1 -1 -1  1  0  0  0  1  1  1  1  0 -1 -1  0  0  0  1  1 -1  0\n",
            "  1  0  1  1  0  1 -1  0  0 -1  0  1 -1 -1  1  1 -1  1 -1  1 -1 -1  1  1\n",
            "  1  0  0  0 -1  1  1 -1  1  1 -1  0  1  1  1 -1  0  1  0  1  1 -1 -1  1\n",
            " -1 -1  1  0  1  0  1 -1  0  1  0 -1  0  1 -1 -1 -1 -1  0  1 -1  0 -1  0\n",
            "  0  1  1  0  1  0  0  1 -1  1  0 -1  0  1  1  0  1  0  1  0  1  0  0  0\n",
            " -1 -1  1 -1  0 -1  1 -1 -1  1 -1  1  0  1  0  0 -1 -1  1 -1  0 -1  1  1\n",
            "  1  0  1 -1 -1 -1  1  0  0  1  0  1  1 -1 -1  1  0 -1  0  0 -1  1  1 -1\n",
            " -1  0  0  0  0  1  1  1  0 -1  0  1  1  1  0  0 -1 -1 -1  1 -1  1 -1  1\n",
            "  1  1 -1 -1  0  1  0  0  1  0  1 -1  1  0 -1 -1  0  1  0 -1  0 -1  1 -1\n",
            " -1 -1  1  1 -1  0  1  1  0  1 -1  0  0 -1  0  1  0  1 -1  1  0  1  1  0\n",
            "  0 -1 -1  1 -1  0  1  1  1  0  0  0  1 -1 -1  1  1 -1  0  1 -1  1 -1  1\n",
            "  0  0  1  1 -1 -1  0  1 -1  1  1  1  0  1  1 -1 -1  1  0 -1  0  1 -1  1\n",
            " -1  0  1  1  1 -1  0 -1  0 -1  1  1  1  1 -1  0  1  1 -1 -1 -1  0 -1 -1\n",
            "  1  1 -1  1  0  1  1  1  0  1  1 -1 -1 -1  1 -1 -1  0  1 -1 -1 -1  0  1\n",
            " -1 -1  1 -1  0  0  1  1  0  1 -1  1  1  0 -1  0  1 -1  1  1 -1  0  1  1\n",
            "  1  0  1 -1  1  1 -1 -1 -1 -1  1  1  0  1  0  1  0  0 -1  1  1  1  1  1\n",
            "  1  0  0  1  1  0 -1  1 -1 -1  1 -1  1  0  1 -1  0  0  1  0 -1 -1  1  1\n",
            "  1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  0 -1  0  1  1  0 -1  1  0  1  1 -1\n",
            " -1  0  0 -1  1  1  0  0  0 -1  0 -1  0  1  0  1 -1 -1 -1  1  0  1  1 -1\n",
            "  1  1 -1  1  1 -1  1  1 -1  1  1  1  0 -1  0 -1 -1  0  0  1  1  0 -1 -1\n",
            "  0 -1  1 -1 -1 -1  0 -1  1  1  1  1  1 -1  0  1 -1 -1  1  0  0  1  0  1\n",
            " -1  1  0  0 -1  1  1  0 -1 -1 -1  0 -1 -1 -1 -1  0  0  0  1 -1  1  1  1\n",
            "  0  1  0 -1 -1  1 -1  1 -1 -1  1  1  1 -1  1  1  1 -1  0 -1  1 -1 -1  0]\n",
            " Number of 1: 331\n",
            " Number of -1's: 278\n",
            "Accuracy  45.27220630372493\n",
            "  Weight 2 (shape (10,)): [ 0  0  0  0 -1  1  0  0 -1 -1]\n",
            " Number of 1: 1\n",
            " Number of -1's: 3\n",
            "Accuracy  100.0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Pruning"
      ],
      "metadata": {
        "id": "eSKVgTIBicRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_1 = tf.keras.models.load_model('baselineWatermarkInband.h5')\n",
        "model = tf.keras.models.load_model('baseline.h5')"
      ],
      "metadata": {
        "id": "6HNLrjj30M88"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QN_H0rqWtdQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pruning_rate= 0.70\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def prune_weights(model, pruning_percentage=0.2):\n",
        "    for layer in model.layers:\n",
        "        if len(layer.get_weights()) > 0:\n",
        "            weights, *biases = layer.get_weights()\n",
        "            threshold = np.percentile(np.abs(weights), pruning_percentage * 100)\n",
        "            new_weights = np.where(np.abs(weights) < threshold, 0, weights)\n",
        "            layer.set_weights([new_weights, *biases])\n",
        "\n",
        "# Example usage:\n",
        "# model = tf.keras.models.load_model('path_to_model')\n",
        "prune_weights(model_1, pruning_percentage = pruning_rate)\n",
        "#prune_weights(model, pruning_percentage=0.90)\n",
        "# To re-evaluate the model's performance after pruning\n",
        "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "# model.evaluate(test_data, test_labels)\n"
      ],
      "metadata": {
        "id": "57ntKEOBidHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation for Raw MNIST Test Data in model trained with Raw data\n",
        "#model_1.save('baseline_watermark.h5')\n",
        "#model_1 = tf.keras.models.load_model('baseline_watermark.h5')\n",
        "print(len(TEST_BASELINE_Y))\n",
        "\n",
        "T_X=copy.deepcopy(TEST_BASELINE_X)\n",
        "T_Y=copy.deepcopy(TEST_BASELINE_Y)\n",
        "\n",
        "T_x = tf.keras.utils.normalize(T_X,axis=1)\n",
        "\n",
        "print(len(T_Y))\n",
        "# If problem appears then convert T_x into np array!\n",
        "\n",
        "val_loss1, val_acc1 = model_1.evaluate(T_x,T_Y)\n",
        "print('loss and accuracy of Raw MNIST Data is',val_loss1,val_acc1)\n",
        "T_X=[]\n",
        "T_x=[]\n",
        "T_Y=[]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKY-o4SQz7ju",
        "outputId": "9683d1a9-3465-4ad3-ebf8-0fb63dd8730a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n",
            "10000\n",
            "313/313 [==============================] - 11s 36ms/step - loss: 0.0370 - accuracy: 0.9942\n",
            "loss and accuracy of Raw MNIST Data is 0.036999478936195374 0.9941999912261963\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(mal_y))\n",
        "\n",
        "T_X=copy.deepcopy(mal_x)\n",
        "T_Y=copy.deepcopy(mal_y)\n",
        "\n",
        "T_x = tf.keras.utils.normalize(T_X,axis=1)\n",
        "\n",
        "print(len(T_Y))\n",
        "# If problem appears then convert T_x into np array!\n",
        "\n",
        "val_loss1, val_acc1 = model_1.evaluate(T_x,T_Y)\n",
        "print('loss and accuracy of Raw MNIST Data is',val_loss1,val_acc1)\n",
        "T_X=[]\n",
        "T_x=[]\n",
        "T_Y=[]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SuuQI1A90DIM",
        "outputId": "2762df1d-674b-4bcd-aa14-4a2844fc9749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6840\n",
            "6840\n",
            "214/214 [==============================] - 2s 9ms/step - loss: 1.5614 - accuracy: 0.6140\n",
            "loss and accuracy of Raw MNIST Data is 1.5613754987716675 0.6140350699424744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Update Model"
      ],
      "metadata": {
        "id": "dezcYxTP7hHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def modify_weights(m1, m2, n):\n",
        "    # Ensure the models have the same architecture\n",
        "    assert len(m1.layers) == len(m2.layers), \"Models must have the same number of layers\"\n",
        "\n",
        "    # Iterate through each layer in the models\n",
        "    for layer1, layer2 in zip(m1.layers, m2.layers):\n",
        "        # Get the weights from both models\n",
        "        weights1 = layer1.get_weights()\n",
        "        weights2 = layer2.get_weights()\n",
        "\n",
        "        # Iterate through each weight matrix/bias vector in the layers\n",
        "        new_weights2 = []\n",
        "        for w1, w2 in zip(weights1, weights2):\n",
        "            # Calculate the absolute difference\n",
        "            abs_diff = np.abs(w1 - w2)\n",
        "\n",
        "            # Decrement the weights of m2 where the absolute difference is >= 1.5\n",
        "            w2 = np.where(abs_diff >= n, w2 - 0.5, w2)\n",
        "\n",
        "            # Append the modified weights to the list\n",
        "            new_weights2.append(w2)\n",
        "\n",
        "        # Set the modified weights back to the layer in m2\n",
        "        layer2.set_weights(new_weights2)\n",
        "    return m2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Modify the weights of m2\n",
        "model_1= modify_weights(model, model_1, 1.5)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sPVSJ_tt0IhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RLcC5QRW8Z1Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}